{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "In order to perform SGHMC in various of problems, the implementation should take functions as arguments to compute gradients of log density functions for various of problems. We intend to enable users to write their own custormized gradient functions in python, and pass them into our sghmc function.\n",
    "\n",
    "Unlike the theory, the algorithm itself is simple. The function takes in user specified gradient functions together with data, initial guess and other parameters in the algorithm. In the sampling process, main computation is computing the gradients and matrices products. Consequently our first implementation is vectorized utilizing numpy matrices. Based on the numpy version, we also tried several optimizations:\n",
    "\n",
    "- Precompilation of gradient functions. When profiling the algorithm, we found that the computation of gradient functions takes a large part in total time. Thus the first optimization we tried is to pass in numba compiled gradient functions.\n",
    "\n",
    "- Rewriting matrices computation in c++. Another heavy computation is updating $r$, which involves several matrices products that will be heavy if the dimension of parameters to sample is large. Thus we pickout the matrices computation part and rewrote it in c++ and wraped it with pybind. Besides optimizing the matrices computation part, we also tried to implement the whole algorithm in c++. However, we found it difficult because the gradient functions could only run in python. These computations are embedded in the sampling process and can not be done in c++ because the gradient functions are user specified.\n",
    "\n",
    "- Applying multiprocess to run multiple simlations simutabeously. We implemented a multithread sghmc function `sghmc_chains` to run multiple independent sampling process.\n",
    "\n",
    "- Removing data shuffle in sampling procedure. SGHMC involves spliting data into mini batches. We found that shuffling data before spliting is a very heavy manipulation, which takes 3/4 of total time. Thus we eliminated the shuffling process in our codes for efficiency.\n",
    "\n",
    "### Timing\n",
    "\n",
    "#### sghmc, sghmc with numba, and sghmc with cpp\n",
    "\n",
    "First we timed the performence of sghmc, sghmc with numba compiled gradient functions, and cpp partwise recoded sghmc function. The problem we use here is simply sampling posterior $\\{\\theta_i\\}$ of a multinormal model: $y_i\\sim N(\\theta_i,1)$ and prior $\\theta_i\\sim N(0,1)$ for $i=1,2,\\dots,ndim$. In all cases we pass in 10,000 simulated observations. The timing result is shown in the following table (time unit: ms):\n",
    "\n",
    "| Problem dimension (ndim) |    sghmc   | sghmc with numba funcs | sghmc with cpp |\n",
    "|:-----------------:|:----------:|:----------------------:|:--------------:|\n",
    "|         10        | 209 ± 4.59 |        233 ± 38        |   182 ± 7.46   |\n",
    "|        100        | 751 ± 60.8 |        691 ± 105       |   496 ± 5.35   |\n",
    "|        1000       | 6290 ± 222 |       4870 ± 291       |   20100 ± 284  |\n",
    "\n",
    "From the table, we can see that\n",
    "\n",
    "- sghmc with numba funcs performs better than sghmc when problem dimension is large.\n",
    "\n",
    "- sghmc with cpp performs better than both python versions when problem dimension is small. However, as problem dimension gets very large, the efficiency of cpp version decreases dramatically.\n",
    "\n",
    "According to the above results, we decided to remove cpp version from our package because of its unstable performance.\n",
    "\n",
    "#### `sghmc` vs. `sghmc_chains`\n",
    "\n",
    "We performed another comparison between `sghmc` and `sghmc_chains` to compare average time on a single chain. We used the same multinormal problem as in above comparison with same data amount and problem dimension equals to 10 and 100 respectively.\n",
    "\n",
    "![a](mpt1.png)\n",
    "\n",
    "![b](mpt2.png)\n",
    "\n",
    "From the comparasion we can see that, for a small problem, simulation for each chain using `sghmc_chains` is efficient than simulation for one chain using `sghmc`. However, when the problem gets heavy, `sghmc_chains` becomes inefficient.\n",
    "\n",
    "We also compared average time on the mixture normal problem (which will be describe in <b>examples</b> section):\n",
    "\n",
    "|          | sghmc | sghmc_chains (20 chains) |\n",
    "|----------|:-----:|:------------------------:|\n",
    "| time(ms) |   98  |       25(498 total)      |\n",
    "\n",
    "We can see that `sghmc_chains` is efficient in simulating large sample in this example.\n",
    "\n",
    "## examples\n",
    "### simulated data\n",
    "->machao\n",
    "\n",
    "### real example: Bayesian Neural Network\n",
    "\n",
    "Due to the complexity of the bayesian convolutional neural network in the original paper, we didn't implement that example. Instead we performed bayesian neural network regression on MPG(https://archive.ics.uci.edu/ml/index.php) data. The purpose of the problem is to predict MPG of a car through other factors. We use SGHMC to sample posterior weights. \n",
    "\n",
    "The neural network has 3 layers: one input layer with 9 input nodes, one hidden layer with 10 nodes, and one output layer with 1 node. The nodes in hidden layer use sigmoid function as activation function. The output of output node is simply weighted sum of its inputs.\n",
    "\n",
    "We give priors $weights\\sim N(0,1)$ and suppose output has distribution $y\\sim N(output,1)$. We run 2000 iterations with fist 100 iterations as heatup.\n",
    "\n",
    "The following figures shows how our network performed on this problem.\n",
    "\n",
    "![c](bnnReg.png)\n",
    "\n",
    "![d](bnnRMSE.png)\n",
    "\n",
    "The first figure is fitted data vs true data on test set. The second figure is the rmse on train and test set respectively along with iterations.\n",
    "\n",
    "The figure shows that our fitted BNN works on test data. The more iterations we run through SGHMC, the more accurate the prediction will be. We can also see that our model didn't overfit the data with in 2000 iterations. And it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
