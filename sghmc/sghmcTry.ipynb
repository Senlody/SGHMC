{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy import linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sghmc(grad_log_den_data, grad_log_den_prior, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = None):\n",
    "    '''\n",
    "    Implementation of Stochastic Gradient Hamiltonian Monte Carlo.\n",
    "    (See details in Chen et al., 2014)\n",
    "    \n",
    "    Dimensions in sampling procdure:\n",
    "        p: dimension of parameters(theta)\n",
    "        n: number of observed data.\n",
    "        m: dimension of data.\n",
    "    \n",
    "    INPUT:            \n",
    "        grad_log_den_data: function with parameters (data,theta)\n",
    "            to compute $\\nabla log(p(data|theta))$ (gradient with respect to theta) of a set of data.\n",
    "            \n",
    "        grad_log_den_prior: function with parameter (theta)\n",
    "            to compute $\\nabla log(p(theta))$.\n",
    "            \n",
    "        data: np.array with shape (n,m)\n",
    "            representing observed data \n",
    "            \n",
    "        V_hat: np.array with shape (p,p)\n",
    "            a matrix of estimated Fisher Information \n",
    "            \n",
    "        eps: float or double\n",
    "            learning rate\n",
    "            \n",
    "        theta_0: np.array with shape (p,)\n",
    "            initial point of sampling.\n",
    "            \n",
    "        C: np.array with shape (p,p)\n",
    "            a matrix representing friction, see paper for details. \n",
    "            C-0.5*eps*V_hat must be positive definite.\n",
    "            \n",
    "        heatup: int\n",
    "            iteration to dump before storing sampling points.\n",
    "            \n",
    "        epoches: int\n",
    "            iterations to run. Must be greater than heatup.\n",
    "        \n",
    "        batch_size: int\n",
    "            size of a minibatch in an iteration, hundreds recommended\n",
    "            \n",
    "        Minv: np.array with shape (p,p)\n",
    "            if default(NULL), will be identical. (See paper for details)\n",
    "            \n",
    "    OUT:\n",
    "        sample: np.array with shape (epoches - heatup, p)\n",
    "            sampled posterior thetas.\n",
    "    '''\n",
    "    \n",
    "    def gradU(grad_log_den_data, grad_log_den_prior, batch, theta, n):\n",
    "        '''\n",
    "        inner function to compute $\\nabla \\tilde{U}$ defined in paper.\n",
    "        '''\n",
    "        return(-(n*grad_log_den_data(batch,theta)/batch.shape[0]+grad_log_den_prior(theta)))\n",
    "    \n",
    "    \n",
    "    n,m = data.shape\n",
    "    p = theta_0.shape[0]\n",
    "    \n",
    "    if(Minv is None):\n",
    "        sqrtM = np.eye(p)\n",
    "        prer = eps\n",
    "        fric = eps*C\n",
    "    else:\n",
    "        sqrtM = la.sqrtm(la.inv(Minv))\n",
    "        prer = eps*Minv\n",
    "        fric = eps*C@Minv\n",
    "\n",
    "    sqrt_noise = la.sqrtm(2*(C-0.5*eps*V_hat)*eps)\n",
    "    \n",
    "    samples = np.zeros((epoches - heatup, p))\n",
    "    batches = np.int(np.ceil(n/batch_size))\n",
    "    \n",
    "    theta = theta_0\n",
    "    for t in range(epoches):\n",
    "        if(Minv is None):\n",
    "            r = np.random.normal(size=(p))\n",
    "        else:\n",
    "            r = sqrtM@np.random.normal(size=(p))\n",
    "        \n",
    "        reorder = random.sample(range(n),n)\n",
    "        for i in range(batches):\n",
    "            batch = data[reorder[(i*batch_size):np.min([(i+1)*batch_size,n])]]\n",
    "            theta = theta + (prer*r if Minv is None else prer@r)\n",
    "            gU = gradU(grad_log_den_data,grad_log_den_prior,batch,theta,n)\n",
    "            r = r - eps*gU - fric@r + sqrt_noise@np.random.normal(size=(p))\n",
    "        theta = theta + (prer*r if Minv is None else prer@r)\n",
    "        \n",
    "        if(t>=heatup):\n",
    "            samples[t-heatup] = theta\n",
    "    \n",
    "    return(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "\n",
    "import cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "import random\n",
    "from scipy import linalg as la\n",
    "\n",
    "def sghmc_cython(grad_log_den_data, grad_log_den_prior, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = None):\n",
    "    '''\n",
    "    Implementation of Stochastic Gradient Hamiltonian Monte Carlo.\n",
    "    (See details in Chen et al., 2014)\n",
    "    \n",
    "    Dimensions in sampling procdure:\n",
    "        p: dimension of parameters(theta)\n",
    "        n: number of observed data.\n",
    "        m: dimension of data.\n",
    "    \n",
    "    INPUT:            \n",
    "        grad_log_den_data: function with parameters (data,theta)\n",
    "            to compute $\\nabla log(p(data|theta))$ (gradient with respect to theta) of a set of data.\n",
    "            \n",
    "        grad_log_den_prior: function with parameter (theta)\n",
    "            to compute $\\nabla log(p(theta))$.\n",
    "            \n",
    "        data: np.array with shape (n,m)\n",
    "            representing observed data \n",
    "            \n",
    "        V_hat: np.array with shape (p,p)\n",
    "            a matrix of estimated Fisher Information \n",
    "            \n",
    "        eps: float or double\n",
    "            learning rate\n",
    "            \n",
    "        theta_0: np.array with shape (p,)\n",
    "            initial point of sampling.\n",
    "            \n",
    "        C: np.array with shape (p,p)\n",
    "            a matrix representing friction, see paper for details. \n",
    "            C-0.5*eps*V_hat must be positive definite.\n",
    "            \n",
    "        heatup: int\n",
    "            iteration to dump before storing sampling points.\n",
    "            \n",
    "        epoches: int\n",
    "            iterations to run. Must be greater than heatup.\n",
    "        \n",
    "        batch_size: int\n",
    "            size of a minibatch in an iteration, hundreds recommended\n",
    "            \n",
    "        Minv: np.array with shape (p,p)\n",
    "            if default(NULL), will be identical. (See paper for details)\n",
    "            \n",
    "    OUT:\n",
    "        sample: np.array with shape (epoches - heatup, p)\n",
    "            sampled posterior thetas.\n",
    "    '''\n",
    "    \n",
    "    def gradU(grad_log_den_data, grad_log_den_prior, batch, theta, n):\n",
    "        '''\n",
    "        inner function to compute $\\nabla \\tilde{U}$ defined in paper.\n",
    "        '''\n",
    "        return(-(n*grad_log_den_data(batch,theta)/batch.shape[0]+grad_log_den_prior(theta)))\n",
    "    \n",
    "    \n",
    "    n,m = data.shape\n",
    "    p = theta_0.shape[0]\n",
    "    \n",
    "    if(Minv is None):\n",
    "        sqrtM = np.eye(p)\n",
    "        prer = eps\n",
    "        fric = eps*C\n",
    "    else:\n",
    "        sqrtM = la.sqrtm(la.inv(Minv))\n",
    "        prer = eps*Minv\n",
    "        fric = eps*C@Minv\n",
    "\n",
    "    sqrt_noise = la.sqrtm(2*(C-0.5*eps*V_hat)*eps)\n",
    "    \n",
    "    samples = np.zeros((epoches - heatup, p))\n",
    "    batches = np.int(np.ceil(n/batch_size))\n",
    "    \n",
    "    theta = theta_0\n",
    "    for t in range(epoches):\n",
    "        if(Minv is None):\n",
    "            r = np.random.normal(size=(p))\n",
    "        else:\n",
    "            r = sqrtM@np.random.normal(size=(p))\n",
    "        \n",
    "        reorder = random.sample(range(n),n)\n",
    "        for i in range(batches):\n",
    "            batch = data[reorder[(i*batch_size):np.min([(i+1)*batch_size,n])]]\n",
    "            theta = theta + (prer*r if Minv is None else prer@r)\n",
    "            gU = gradU(grad_log_den_data,grad_log_den_prior,batch,theta,n)\n",
    "            r = r - eps*gU - fric@r + sqrt_noise@np.random.normal(size=(p))\n",
    "        theta = theta + (prer*r if Minv is None else prer@r)\n",
    "        \n",
    "        if(t>=heatup):\n",
    "            samples[t-heatup] = theta\n",
    "    \n",
    "    return(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "#@jit('double[:](double[:](double[:,:],double[:]),double[:](double[:]),double[:,:],double[:],int32)')\n",
    "@jit\n",
    "def gradU_jit(grad_log_den_data, grad_log_den_prior, batch, theta, n):\n",
    "    '''\n",
    "    inner function to compute $\\nabla \\tilde{U}$ defined in paper.\n",
    "    '''\n",
    "    return(-(n*grad_log_den_data(batch,theta)/batch.shape[0]+grad_log_den_prior(theta)))\n",
    "\n",
    "@jit\n",
    "def sghmc_jit(grad_log_den_data, grad_log_den_prior, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = None):\n",
    "    '''\n",
    "    Implementation of Stochastic Gradient Hamiltonian Monte Carlo.\n",
    "    (See details in Chen et al., 2014)\n",
    "    \n",
    "    Dimensions in sampling procdure:\n",
    "        p: dimension of parameters(theta)\n",
    "        n: number of observed data.\n",
    "        m: dimension of data.\n",
    "    \n",
    "    INPUT:            \n",
    "        grad_log_den_data: function with parameters (data,theta)\n",
    "            to compute $\\nabla log(p(data|theta))$ (gradient with respect to theta) of a set of data.\n",
    "            \n",
    "        grad_log_den_prior: function with parameter (theta)\n",
    "            to compute $\\nabla log(p(theta))$.\n",
    "            \n",
    "        data: np.array with shape (n,m)\n",
    "            representing observed data \n",
    "            \n",
    "        V_hat: np.array with shape (p,p)\n",
    "            a matrix of estimated Fisher Information \n",
    "            \n",
    "        eps: float or double\n",
    "            learning rate\n",
    "            \n",
    "        theta_0: np.array with shape (p,)\n",
    "            initial point of sampling.\n",
    "            \n",
    "        C: np.array with shape (p,p)\n",
    "            a matrix representing friction, see paper for details. \n",
    "            C-0.5*eps*V_hat must be positive definite.\n",
    "            \n",
    "        heatup: int\n",
    "            iteration to dump before storing sampling points.\n",
    "            \n",
    "        epoches: int\n",
    "            iterations to run. Must be greater than heatup.\n",
    "        \n",
    "        batch_size: int\n",
    "            size of a minibatch in an iteration, hundreds recommended\n",
    "            \n",
    "        Minv: np.array with shape (p,p)\n",
    "            if default(NULL), will be identical. (See paper for details)\n",
    "            \n",
    "    OUT:\n",
    "        sample: np.array with shape (epoches - heatup, p)\n",
    "            sampled posterior thetas.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    n,m = data.shape\n",
    "    p = theta_0.shape[0]\n",
    "    \n",
    "    if(Minv is None):\n",
    "        sqrtM = np.eye(p)\n",
    "        prer = eps\n",
    "        fric = eps*C\n",
    "    else:\n",
    "        sqrtM = la.sqrtm(la.inv(Minv))\n",
    "        prer = eps*Minv\n",
    "        fric = eps*C@Minv\n",
    "\n",
    "    sqrt_noise = la.sqrtm(2*(C-0.5*eps*V_hat)*eps)\n",
    "    \n",
    "    samples = np.zeros((epoches - heatup, p))\n",
    "    batches = np.int(np.ceil(n/batch_size))\n",
    "    \n",
    "    theta = theta_0\n",
    "    for t in range(epoches):\n",
    "        if(Minv is None):\n",
    "            r = np.random.normal(size=(p))\n",
    "        else:\n",
    "            r = sqrtM@np.random.normal(size=(p))\n",
    "        \n",
    "        reorder = random.sample(range(n),n)\n",
    "        for i in range(batches):\n",
    "            batch = data[reorder[(i*batch_size):np.min([(i+1)*batch_size,n])]]\n",
    "            theta = theta + (prer*r if Minv is None else prer@r)\n",
    "            gU = gradU_jit(grad_log_den_data,grad_log_den_prior,batch,theta,n)\n",
    "            r = r - eps*gU - fric@r + sqrt_noise@np.random.normal(size=(p))\n",
    "        theta = theta + (prer*r if Minv is None else prer@r)\n",
    "        \n",
    "        if(t>=heatup):\n",
    "            samples[t-heatup] = theta\n",
    "    \n",
    "    return(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_log_den_data(data,theta):\n",
    "    return(np.sum(data-theta,axis=0))\n",
    "\n",
    "def grad_log_den_prior(theta):\n",
    "    return(np.array([0,0])-theta)\n",
    "\n",
    "data = np.array([-10,10])+np.random.normal(size=(100,2))\n",
    "V_hat=np.eye(2)\n",
    "eps=0.01\n",
    "theta_0=np.zeros(2)\n",
    "C=np.eye(2)\n",
    "heatup=100\n",
    "epoches=200\n",
    "batch_size=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sghmc(grad_log_den_data, grad_log_den_prior, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sghmc(grad_log_den_data, grad_log_den_prior, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = np.eye(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.8 ms ± 1.48 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "48.3 ms ± 1.9 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "46.4 ms ± 616 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "80.7 ms ± 4.94 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc(grad_log_den_data, grad_log_den_prior, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = None)\n",
    "%timeit sghmc(grad_log_den_data, grad_log_den_prior, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = np.eye(2))\n",
    "%timeit sghmc_cython(grad_log_den_data, grad_log_den_prior, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = None)\n",
    "%timeit sghmc_jit(grad_log_den_data, grad_log_den_prior, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 ms ± 1.56 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "gldd_jit=numba.jit(grad_log_den_data)\n",
    "gldp_jit=numba.jit(grad_log_den_prior)\n",
    "%timeit sghmc_jit(gldd_jit, gldp_jit, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=sghmc(grad_log_den_data, grad_log_den_prior, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = np.eye(2))\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(*sample.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
