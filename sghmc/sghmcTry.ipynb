{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy import linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sghmc(grad_log_den_data, grad_log_den_prior, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = None):\n",
    "    '''\n",
    "    Implementation of Stochastic Gradient Hamiltonian Monte Carlo.\n",
    "    (See details in Chen et al., 2014)\n",
    "    \n",
    "    Dimensions in sampling procdure:\n",
    "        p: dimension of parameters(theta)\n",
    "        n: number of observed data.\n",
    "        m: dimension of data.\n",
    "    \n",
    "    INPUT:            \n",
    "        grad_log_den_data: function with parameters (data,theta)\n",
    "            to compute $\\nabla log(p(data|theta))$ (gradient with respect to theta) of a set of data.\n",
    "            \n",
    "        grad_log_den_prior: function with parameter (theta)\n",
    "            to compute $\\nabla log(p(theta))$.\n",
    "            \n",
    "        data: np.array with shape (n,m)\n",
    "            representing observed data \n",
    "            \n",
    "        V_hat: np.array with shape (p,p)\n",
    "            a matrix of estimated Fisher Information \n",
    "            \n",
    "        eps: float or double\n",
    "            learning rate\n",
    "            \n",
    "        theta_0: np.array with shape (p,)\n",
    "            initial point of sampling.\n",
    "            \n",
    "        C: np.array with shape (p,p)\n",
    "            a matrix representing friction, see paper for details. \n",
    "            C-0.5*eps*V_hat must be positive definite.\n",
    "            \n",
    "        heatup: int\n",
    "            iteration to dump before storing sampling points.\n",
    "            \n",
    "        epoches: int\n",
    "            iterations to run. Must be greater than heatup.\n",
    "        \n",
    "        batch_size: int\n",
    "            size of a minibatch in an iteration, hundreds recommended\n",
    "            \n",
    "        Minv: np.array with shape (p,p)\n",
    "            if default(NULL), will be identical. (See paper for details)\n",
    "            \n",
    "    OUT:\n",
    "        sample: np.array with shape (epoches - heatup, p)\n",
    "            sampled posterior thetas.\n",
    "    '''\n",
    "    \n",
    "    def gradU(grad_log_den_data, grad_log_den_prior, batch, theta, n):\n",
    "        '''\n",
    "        inner function to compute $\\nabla \\tilde{U}$ defined in paper.\n",
    "        '''\n",
    "        return(-(n*grad_log_den_data(batch,theta)/batch.shape[0]+grad_log_den_prior(theta)))\n",
    "    \n",
    "    \n",
    "    n,m = data.shape\n",
    "    p = theta_0.shape[0]\n",
    "    \n",
    "    if(Minv is None):\n",
    "        sqrtM = np.eye(p)\n",
    "        prer = eps\n",
    "        fric = eps*C\n",
    "    else:\n",
    "        sqrtM = la.sqrtm(la.inv(Minv))\n",
    "        prer = eps*Minv\n",
    "        fric = eps*C@Minv\n",
    "\n",
    "    sqrt_noise = la.sqrtm(2*(C-0.5*eps*V_hat)*eps)\n",
    "    \n",
    "    samples = np.zeros((epoches - heatup, p))\n",
    "    batches = np.int(np.ceil(n/batch_size))\n",
    "    \n",
    "    theta = theta_0\n",
    "    for t in range(epoches):\n",
    "        if(Minv is None):\n",
    "            r = np.random.normal(size=(p))\n",
    "        else:\n",
    "            r = sqrtM@np.random.normal(size=(p))\n",
    "        \n",
    "        split = np.split(data,batches)\n",
    "        for i in range(batches):\n",
    "            batch = split[i]\n",
    "            theta = theta + (prer*r if Minv is None else prer@r)\n",
    "            gU = gradU(grad_log_den_data,grad_log_den_prior,batch,theta,n)\n",
    "            r = r - eps*gU - fric@r + sqrt_noise@np.random.normal(size=(p))\n",
    "        theta = theta + (prer*r if Minv is None else prer@r)\n",
    "        \n",
    "        if(t>=heatup):\n",
    "            samples[t-heatup] = theta\n",
    "    \n",
    "    return(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sghmc_jit(grad_log_den_data, grad_log_den_prior, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = None):\n",
    "    '''\n",
    "    Implementation of Stochastic Gradient Hamiltonian Monte Carlo.\n",
    "    (See details in Chen et al., 2014)\n",
    "    \n",
    "    Dimensions in sampling procdure:\n",
    "        p: dimension of parameters(theta)\n",
    "        n: number of observed data.\n",
    "        m: dimension of data.\n",
    "    \n",
    "    INPUT:            \n",
    "        grad_log_den_data: function with parameters (data,theta)\n",
    "            to compute $\\nabla log(p(data|theta))$ (gradient with respect to theta) of a set of data.\n",
    "            \n",
    "        grad_log_den_prior: function with parameter (theta)\n",
    "            to compute $\\nabla log(p(theta))$.\n",
    "            \n",
    "        data: np.array with shape (n,m)\n",
    "            representing observed data \n",
    "            \n",
    "        V_hat: np.array with shape (p,p)\n",
    "            a matrix of estimated Fisher Information \n",
    "            \n",
    "        eps: float or double\n",
    "            learning rate\n",
    "            \n",
    "        theta_0: np.array with shape (p,)\n",
    "            initial point of sampling.\n",
    "            \n",
    "        C: np.array with shape (p,p)\n",
    "            a matrix representing friction, see paper for details. \n",
    "            C-0.5*eps*V_hat must be positive definite.\n",
    "            \n",
    "        heatup: int\n",
    "            iteration to dump before storing sampling points.\n",
    "            \n",
    "        epoches: int\n",
    "            iterations to run. Must be greater than heatup.\n",
    "        \n",
    "        batch_size: int\n",
    "            size of a minibatch in an iteration, hundreds recommended\n",
    "            \n",
    "        Minv: np.array with shape (p,p)\n",
    "            if default(NULL), will be identical. (See paper for details)\n",
    "            \n",
    "    OUT:\n",
    "        sample: np.array with shape (epoches - heatup, p)\n",
    "            sampled posterior thetas.\n",
    "    '''\n",
    "    \n",
    "    def gradU(grad_log_den_data, grad_log_den_prior, batch, theta, n):\n",
    "        '''\n",
    "        inner function to compute $\\nabla \\tilde{U}$ defined in paper.\n",
    "        '''\n",
    "        return(-(n*grad_log_den_data(batch,theta)/batch.shape[0]+grad_log_den_prior(theta)))\n",
    "\n",
    "    n,m = data.shape\n",
    "    p = theta_0.shape[0]\n",
    "    \n",
    "    if(Minv is None):\n",
    "        sqrtM = np.eye(p)\n",
    "        prer = eps\n",
    "        fric = eps*C\n",
    "    else:\n",
    "        sqrtM = la.sqrtm(la.inv(Minv))\n",
    "        prer = eps*Minv\n",
    "        fric = eps*C@Minv\n",
    "\n",
    "    sqrt_noise = la.sqrtm(2*(C-0.5*eps*V_hat)*eps)\n",
    "    \n",
    "    samples = np.zeros((epoches - heatup, p))\n",
    "    batches = np.int(np.ceil(n/batch_size))\n",
    "    \n",
    "    theta = theta_0\n",
    "    for t in range(epoches):\n",
    "        if(Minv is None):\n",
    "            r = np.random.normal(size=(p))\n",
    "        else:\n",
    "            r = sqrtM@np.random.normal(size=(p))\n",
    "        \n",
    "        split = np.split(data,batches)\n",
    "        for i in range(batches):\n",
    "            batch = split[i]\n",
    "            theta = theta + (prer*r if Minv is None else prer@r)\n",
    "            gU = gradU(grad_log_den_data,grad_log_den_prior,batch,theta,n)\n",
    "            r = r - eps*gU - fric@r + sqrt_noise@np.random.normal(size=(p))\n",
    "        theta = theta + (prer*r if Minv is None else prer@r)\n",
    "        \n",
    "        if(t>=heatup):\n",
    "            samples[t-heatup] = theta\n",
    "    \n",
    "    return(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cppfuncs.cpp\n"
     ]
    }
   ],
   "source": [
    "%%file cppfuncs.cpp\n",
    "<%\n",
    "cfg['include_dirs'] = ['eigen']\n",
    "setup_pybind11(cfg)\n",
    "%>\n",
    "#include <pybind11/pybind11.h>\n",
    "#include <pybind11/stl.h>\n",
    "#include <pybind11/eigen.h>\n",
    "#include <vector>\n",
    "#include <random>\n",
    "#include <Eigen/Dense>\n",
    "#include <functional>\n",
    "\n",
    "namespace py = pybind11;\n",
    "using std::default_random_engine;\n",
    "using std::normal_distribution;\n",
    "using std::bind;\n",
    "using namespace Eigen;\n",
    "    \n",
    "//#@jit('double[:](double[:],double[:],double,double[:,:],double[:,:],int32)')\n",
    "//#def update_r(r,gU,eps,fric,sqrt_noise,p):\n",
    "//#    return r - eps*gU - fric@r + sqrt_noise@np.random.normal(size=(p))\n",
    "\n",
    "default_random_engine re{};\n",
    "normal_distribution<double> norm(0,1);\n",
    "auto rnorm = bind(norm, re);\n",
    "\n",
    "VectorXd update_r(VectorXd r, VectorXd gU,double eps,MatrixXd fric,MatrixXd sqrt_noise){\n",
    "    int p = r.rows();\n",
    "    VectorXd noise = VectorXd::Zero(p).unaryExpr([](double x){ return rnorm();});\n",
    "    return r.array() - eps*gU.array() - (fric*r).array() + (sqrt_noise*noise).array();\n",
    "}\n",
    "\n",
    "PYBIND11_MODULE(cppfuncs, m) {\n",
    "    m.def(\"update_r\", &update_r);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg as la\n",
    "import cppimport\n",
    "cppfuncs = cppimport.imp(\"cppfuncs\")\n",
    "\n",
    "def sghmc_cpp(grad_log_den_data, grad_log_den_prior, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = None):\n",
    "    '''\n",
    "    Implementation of Stochastic Gradient Hamiltonian Monte Carlo.\n",
    "    (See details in Chen et al., 2014)\n",
    "    \n",
    "    Dimensions in sampling procdure:\n",
    "        p: dimension of parameters(theta)\n",
    "        n: number of observed data.\n",
    "        m: dimension of data.\n",
    "    \n",
    "    INPUT:            \n",
    "        grad_log_den_data: function with parameters (data,theta)\n",
    "            to compute $\\nabla log(p(data|theta))$ (gradient with respect to theta) of a set of data.\n",
    "            \n",
    "        grad_log_den_prior: function with parameter (theta)\n",
    "            to compute $\\nabla log(p(theta))$.\n",
    "            \n",
    "        data: np.array with shape (n,m)\n",
    "            representing observed data \n",
    "            \n",
    "        V_hat: np.array with shape (p,p)\n",
    "            a matrix of estimated Fisher Information \n",
    "            \n",
    "        eps: float or double\n",
    "            learning rate\n",
    "            \n",
    "        theta_0: np.array with shape (p,)\n",
    "            initial point of sampling.\n",
    "            \n",
    "        C: np.array with shape (p,p)\n",
    "            a matrix representing friction, see paper for details. \n",
    "            C-0.5*eps*V_hat must be positive definite.\n",
    "            \n",
    "        heatup: int\n",
    "            iteration to dump before storing sampling points.\n",
    "            \n",
    "        epoches: int\n",
    "            iterations to run. Must be greater than heatup.\n",
    "        \n",
    "        batch_size: int\n",
    "            size of a minibatch in an iteration, hundreds recommended\n",
    "            \n",
    "        Minv: np.array with shape (p,p)\n",
    "            if default(NULL), will be identical. (See paper for details)\n",
    "            \n",
    "    OUT:\n",
    "        sample: np.array with shape (epoches - heatup, p)\n",
    "            sampled posterior thetas.\n",
    "    '''\n",
    "    \n",
    "    def gradU(grad_log_den_data, grad_log_den_prior, batch, theta, n):\n",
    "        '''\n",
    "        inner function to compute $\\nabla \\tilde{U}$ defined in paper.\n",
    "        '''\n",
    "        return(-(n*grad_log_den_data(batch,theta)/batch.shape[0]+grad_log_den_prior(theta)))\n",
    "\n",
    "    n,m = data.shape\n",
    "    p = theta_0.shape[0]\n",
    "    \n",
    "    if(Minv is None):\n",
    "        sqrtM = np.eye(p)\n",
    "        prer = eps\n",
    "        fric = eps*C\n",
    "    else:\n",
    "        sqrtM = la.sqrtm(la.inv(Minv))\n",
    "        prer = eps*Minv\n",
    "        fric = eps*C@Minv\n",
    "\n",
    "    sqrt_noise = la.sqrtm(2*(C-0.5*eps*V_hat)*eps)\n",
    "    \n",
    "    samples = np.zeros((epoches - heatup, p))\n",
    "    batches = np.int(np.ceil(n/batch_size))\n",
    "    \n",
    "    theta = theta_0\n",
    "    for t in range(epoches):\n",
    "        if(Minv is None):\n",
    "            r = np.random.normal(size=(p))\n",
    "        else:\n",
    "            r = sqrtM@np.random.normal(size=(p))\n",
    "        \n",
    "        split = np.split(data,batches)\n",
    "        for i in range(batches):\n",
    "            batch = split[i]\n",
    "            theta = theta + (prer*r if Minv is None else prer@r)\n",
    "            gU = gradU(grad_log_den_data,grad_log_den_prior,batch,theta,n)\n",
    "            r = cppfuncs.update_r(r,gU,eps,fric,sqrt_noise)\n",
    "        theta = theta + (prer*r if Minv is None else prer@r)\n",
    "        \n",
    "        if(t>=heatup):\n",
    "            samples[t-heatup] = theta\n",
    "    \n",
    "    return(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg as la\n",
    "import cppimport\n",
    "cppfuncs = cppimport.imp(\"cppfuncs\")\n",
    "\n",
    "def sghmc_cpp_noM(grad_log_den_data, grad_log_den_prior, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size):\n",
    "    '''\n",
    "    Implementation of Stochastic Gradient Hamiltonian Monte Carlo.\n",
    "    (See details in Chen et al., 2014)\n",
    "    \n",
    "    Dimensions in sampling procdure:\n",
    "        p: dimension of parameters(theta)\n",
    "        n: number of observed data.\n",
    "        m: dimension of data.\n",
    "    \n",
    "    INPUT:            \n",
    "        grad_log_den_data: function with parameters (data,theta)\n",
    "            to compute $\\nabla log(p(data|theta))$ (gradient with respect to theta) of a set of data.\n",
    "            \n",
    "        grad_log_den_prior: function with parameter (theta)\n",
    "            to compute $\\nabla log(p(theta))$.\n",
    "            \n",
    "        data: np.array with shape (n,m)\n",
    "            representing observed data \n",
    "            \n",
    "        V_hat: np.array with shape (p,p)\n",
    "            a matrix of estimated Fisher Information \n",
    "            \n",
    "        eps: float or double\n",
    "            learning rate\n",
    "            \n",
    "        theta_0: np.array with shape (p,)\n",
    "            initial point of sampling.\n",
    "            \n",
    "        C: np.array with shape (p,p)\n",
    "            a matrix representing friction, see paper for details. \n",
    "            C-0.5*eps*V_hat must be positive definite.\n",
    "            \n",
    "        heatup: int\n",
    "            iteration to dump before storing sampling points.\n",
    "            \n",
    "        epoches: int\n",
    "            iterations to run. Must be greater than heatup.\n",
    "        \n",
    "        batch_size: int\n",
    "            size of a minibatch in an iteration, hundreds recommended\n",
    "            \n",
    "        Minv: np.array with shape (p,p)\n",
    "            if default(NULL), will be identical. (See paper for details)\n",
    "            \n",
    "    OUT:\n",
    "        sample: np.array with shape (epoches - heatup, p)\n",
    "            sampled posterior thetas.\n",
    "    '''\n",
    "    \n",
    "    def gradU(grad_log_den_data, grad_log_den_prior, batch, theta, n):\n",
    "        '''\n",
    "        inner function to compute $\\nabla \\tilde{U}$ defined in paper.\n",
    "        '''\n",
    "        return(-(n*grad_log_den_data(batch,theta)/batch.shape[0]+grad_log_den_prior(theta)))\n",
    "\n",
    "    n,m = data.shape\n",
    "    p = theta_0.shape[0]\n",
    "\n",
    "    fric = eps*C\n",
    "\n",
    "    sqrt_noise = la.sqrtm(2*(C-0.5*eps*V_hat)*eps)\n",
    "    \n",
    "    samples = np.zeros((epoches - heatup, p))\n",
    "    batches = np.int(np.ceil(n/batch_size))\n",
    "    \n",
    "    theta = theta_0\n",
    "    for t in range(epoches):\n",
    "        r = np.random.normal(size=(p))\n",
    "        \n",
    "        split = np.split(data,batches)\n",
    "        for i in range(batches):\n",
    "            batch = split[i]\n",
    "            theta = theta + eps*r\n",
    "            gU = gradU(grad_log_den_data,grad_log_den_prior,batch,theta,n)\n",
    "            r = cppfuncs.update_r(r,gU,eps,fric,sqrt_noise)\n",
    "        theta = theta + eps*r\n",
    "        \n",
    "        if(t>=heatup):\n",
    "            samples[t-heatup] = theta\n",
    "    \n",
    "    return(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cpp opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(list(range(100)))+2*np.random.normal(size=(10000,100))\n",
    "V_hat=np.eye(100)\n",
    "eps=0.01\n",
    "theta_0=np.zeros(100)\n",
    "C=np.eye(100)\n",
    "heatup=100\n",
    "epoches=200\n",
    "batch_size=500\n",
    "\n",
    "def gradU(grad_log_den_data, grad_log_den_prior, batch, theta, n):\n",
    "        '''\n",
    "        inner function to compute $\\nabla \\tilde{U}$ defined in paper.\n",
    "        '''\n",
    "        return(-(n*grad_log_den_data(batch,theta)/batch.shape[0]+grad_log_den_prior(theta)))\n",
    "\n",
    "n,m = data.shape\n",
    "p = theta_0.shape[0]\n",
    "\n",
    "fric = eps*C\n",
    "\n",
    "sqrt_noise = la.sqrtm(2*(C-0.5*eps*V_hat)*eps)\n",
    "\n",
    "samples = np.zeros((epoches - heatup, p))\n",
    "batches = np.int(np.ceil(n/batch_size))\n",
    "\n",
    "theta = theta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.normal(size=(p))\n",
    "        \n",
    "split = np.split(data,batches)\n",
    "for i in range(batches):\n",
    "    batch = split[i]\n",
    "    theta = theta + eps*r\n",
    "    gU = gradU(grad_log_den_data,grad_log_den_prior,batch,theta,n)\n",
    "    r = cppfuncs.update_r(r,gU,eps,fric,sqrt_noise)\n",
    "theta = theta + eps*r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.81 µs ± 29.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "35.3 µs ± 548 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "1.09 µs ± 4.67 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n",
      "89.7 µs ± 289 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "29.3 µs ± 98.5 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.random.normal(size=(p))\n",
    "%timeit np.split(data,batches)\n",
    "%timeit theta + eps*r\n",
    "%timeit gradU(grad_log_den_data,grad_log_den_prior,batch,theta,n)\n",
    "%timeit cppfuncs.update_r(r,gU,eps,fric,sqrt_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 µs ± 507 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit gradU(grad_log_den_data_jit,grad_log_den_prior_jit,batch,theta,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.7 µs ± 814 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit r - eps*gU - fric@r + sqrt_noise@np.random.normal(size=(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_log_den_data(data,theta):\n",
    "    return(np.sum(data-theta,axis=0))\n",
    "\n",
    "def grad_log_den_prior(theta):\n",
    "    return(-theta)\n",
    "\n",
    "import numba\n",
    "from numba import jit\n",
    "@jit('double[:](double[:,:],double[:])')\n",
    "def grad_log_den_data_jit(data,theta):\n",
    "    return(np.sum(data-theta,axis=0))\n",
    "\n",
    "@jit('double[:](double[:])')\n",
    "def grad_log_den_prior_jit(theta):\n",
    "    return(-theta)\n",
    "\n",
    "dnorm=1000\n",
    "data = np.array(list(range(dnorm)))+2*np.random.normal(size=(10000,dnorm))\n",
    "V_hat=np.eye(dnorm)\n",
    "eps=0.01\n",
    "theta_0=np.zeros(dnorm)\n",
    "C=np.eye(dnorm)\n",
    "heatup=100\n",
    "epoches=200\n",
    "batch_size=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.01 s ± 62 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "3.89 s ± 52.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "19.1 s ± 107 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc(grad_log_den_data, grad_log_den_prior, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = None)\n",
    "%timeit sghmc_jit(grad_log_den_data_jit, grad_log_den_prior_jit, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = None)\n",
    "%timeit sghmc_cpp(grad_log_den_data_jit, grad_log_den_prior_jit, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492 ms ± 4.29 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_cpp_noM(grad_log_den_data_jit, grad_log_den_prior_jit, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.15807424,  1.01034599,  1.83609643, ..., 97.05898675,\n",
       "        98.04735348, 98.97711129],\n",
       "       [-0.18926992,  0.99169964,  1.88192535, ..., 96.93019707,\n",
       "        97.9011416 , 98.88606449],\n",
       "       [-0.1530833 ,  1.01816102,  1.83819022, ..., 97.03696292,\n",
       "        98.0415609 , 98.97156504],\n",
       "       ...,\n",
       "       [-0.17963601,  1.01113095,  1.87209616, ..., 96.95619933,\n",
       "        97.98494918, 98.93137916],\n",
       "       [-0.16956746,  0.987409  ,  1.84164769, ..., 97.01577784,\n",
       "        97.97021751, 98.93303963],\n",
       "       [-0.17911804,  1.01706299,  1.8731486 , ..., 96.96036453,\n",
       "        97.98539675, 98.93018898]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sghmc(grad_log_den_data, grad_log_den_prior, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sghmc(grad_log_den_data, grad_log_den_prior, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = np.eye(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=sghmc(grad_log_den_data, grad_log_den_prior, data, V_hat, eps, theta_0, C, heatup, epoches, batch_size, Minv = np.eye(2))\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(*sample.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
